{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47271973-cf2a-464a-9875-9f83f929f222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' You can use the below code to get the S3 bucket to write any artifacts to\\n    ```\\n    import sagemaker\\n    session = sagemaker.Session()\\n    bucket = session.default_bucket()\\n    ```\\n- What ML task is this? Classification? Regression? Clustering?\\n- What are the data types of the columns? What pre-processing should you apply?\\n- How to determine the best hyperparameters for the model?\\n- How to test if the model is deployed successfully?'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Wine Type Prediction\n",
    "''' [The Wine Type Prediction dataset](https://archive.ics.uci.edu/ml/datasets/Wine) consists of data related to the chemical properties of various wines and classifies each wine into on1 of 3 possible classes. The columns in the dataset are as follows:\n",
    "\n",
    "|col name|description|\n",
    "|:--|:--|\n",
    "|target| This is the target variable to be predicted. There are three possible classes, class 1, 2 and 3 |\n",
    "|alcohol| continuous | \n",
    "|malic_acid| continuous | \n",
    "|ash| continuous | \n",
    "|alcalinity_of_ash| continuous |    \n",
    "|magnesium| continuous | \n",
    "|total_phenols| continuous | \n",
    "|flavanoids| continuous | \n",
    "|nonflavanoid_phenols| continuous | \n",
    "|proanthocyanins| continuous | \n",
    "|color_intensity| continuous | \n",
    "|hue| continuous | \n",
    "|od280/od315_of_diluted_wines| continuous | \n",
    "|proline| continuous | \n",
    "\n",
    "\n",
    "- The goal of this project is to build and tune a model to predict the `target` column using AWS Sagemaker and deploy the model as a `Serverless Inference Endpoint`\n",
    "'''\n",
    "## Tips: \n",
    "''' You can use the below code to get the S3 bucket to write any artifacts to\n",
    "    ```\n",
    "    import sagemaker\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    ```\n",
    "- What ML task is this? Classification? Regression? Clustering?\n",
    "- What are the data types of the columns? What pre-processing should you apply?\n",
    "- How to determine the best hyperparameters for the model?\n",
    "- How to test if the model is deployed successfully?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40107915-9f96-473b-ac35-f6e783d6a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cols = [\n",
    "    \"target\",\n",
    "    \"alcohol\", \n",
    "    \"malic_acid\", \n",
    "    \"ash\", \n",
    "    \"alcalinity_of_ash\",    \n",
    "    \"magnesium\", \n",
    "    \"total_phenols\", \n",
    "    \"flavanoids\", \n",
    "    \"nonflavanoid_phenols\", \n",
    "    \"proanthocyanins\", \n",
    "    \"color_intensity\", \n",
    "    \"hue\", \n",
    "    \"od280/od315_of_diluted_wines\", \n",
    "    \"proline\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90cb45a6-01c1-4f9e-a5ad-5e0b549dbbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target  alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  \\\n",
       "0       1    14.23        1.71  2.43               15.6        127   \n",
       "1       1    13.20        1.78  2.14               11.2        100   \n",
       "2       1    13.16        2.36  2.67               18.6        101   \n",
       "3       1    14.37        1.95  2.50               16.8        113   \n",
       "4       1    13.24        2.59  2.87               21.0        118   \n",
       "\n",
       "   total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  \\\n",
       "0           2.80        3.06                  0.28             2.29   \n",
       "1           2.65        2.76                  0.26             1.28   \n",
       "2           2.80        3.24                  0.30             2.81   \n",
       "3           3.85        3.49                  0.24             2.18   \n",
       "4           2.80        2.69                  0.39             1.82   \n",
       "\n",
       "   color_intensity   hue  od280/od315_of_diluted_wines  proline  \n",
       "0             5.64  1.04                          3.92     1065  \n",
       "1             4.38  1.05                          3.40     1050  \n",
       "2             5.68  1.03                          3.17     1185  \n",
       "3             7.80  0.86                          3.45     1480  \n",
       "4             4.32  1.04                          2.93      735  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", names=cols)\n",
    "\n",
    "print(wine_df.shape)\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d6f55f-2582-4fb6-97ec-24354906c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = wine_df.drop(columns=['target'])\n",
    "y = wine_df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=42, stratify=y)\n",
    "\n",
    "# Concatenate features and target variable for training and testing sets\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "test_df = pd.concat([X_test, y_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe5de24-f216-43ef-8c6f-403e65516d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>12.52</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.78</td>\n",
       "      <td>325</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>13.73</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.70</td>\n",
       "      <td>22.5</td>\n",
       "      <td>101</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1285</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13.28</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.84</td>\n",
       "      <td>15.5</td>\n",
       "      <td>110</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.78</td>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>14.34</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.70</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.96</td>\n",
       "      <td>660</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.5</td>\n",
       "      <td>81</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.27</td>\n",
       "      <td>480</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.70</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.71</td>\n",
       "      <td>660</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>13.51</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.54</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>13.69</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.54</td>\n",
       "      <td>20.0</td>\n",
       "      <td>107</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.82</td>\n",
       "      <td>680</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>12.82</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.30</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.26</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>685</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "111    12.52        2.43  2.17               21.0         88           2.55   \n",
       "30     13.73        1.50  2.70               22.5        101           3.00   \n",
       "36     13.28        1.64  2.84               15.5        110           2.60   \n",
       "12     13.75        1.73  2.41               16.0         89           2.60   \n",
       "158    14.34        1.68  2.70               25.0         98           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "90     12.08        1.83  2.32               18.5         81           1.60   \n",
       "172    14.16        2.51  2.48               20.0         91           1.68   \n",
       "34     13.51        1.80  2.65               19.0        110           2.35   \n",
       "161    13.69        3.26  2.54               20.0        107           1.83   \n",
       "167    12.82        3.37  2.30               19.5         88           1.48   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "111        2.27                  0.26             1.22             2.00  0.90   \n",
       "30         3.25                  0.29             2.38             5.70  1.19   \n",
       "36         2.68                  0.34             1.36             4.60  1.09   \n",
       "12         2.76                  0.29             1.81             5.60  1.15   \n",
       "158        1.31                  0.53             2.70            13.00  0.57   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "90         1.50                  0.52             1.64             2.40  1.08   \n",
       "172        0.70                  0.44             1.24             9.70  0.62   \n",
       "34         2.53                  0.29             1.54             4.20  1.10   \n",
       "161        0.56                  0.50             0.80             5.88  0.96   \n",
       "167        0.66                  0.40             0.97            10.26  0.72   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "111                          2.78      325       2  \n",
       "30                           2.71     1285       1  \n",
       "36                           2.78      880       1  \n",
       "12                           2.90     1320       1  \n",
       "158                          1.96      660       3  \n",
       "..                            ...      ...     ...  \n",
       "90                           2.27      480       2  \n",
       "172                          1.71      660       3  \n",
       "34                           2.87     1095       1  \n",
       "161                          1.82      680       3  \n",
       "167                          1.75      685       3  \n",
       "\n",
       "[138 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7d6145-a296-492b-beff-95a255c2bd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker-us-east-1-635439539142\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d94c33-fa1d-4b85-9c6c-75f260c3edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_json('../data/wine_test_df_json.json',orient='records',lines=True,index=False)\n",
    "train_df.to_csv('../data/train_data_wine_pred.csv')\n",
    "test_df.to_csv('../data/test_data_wine_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e55465-71fb-4f1c-941a-a3855c27e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train path: s3://sagemaker-us-east-1-635439539142/akash/wine-prediction/train_data_wine_pred.csv\n",
      "test path: s3://sagemaker-us-east-1-635439539142/akash/wine-prediction/test_data_wine_pred.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = session.upload_data(path='../data/train_data_wine_pred.csv', bucket=bucket, key_prefix = 'akash/wine-prediction')\n",
    "test_path = session.upload_data(path='../data/test_data_wine_pred.csv', bucket=bucket, key_prefix = 'akash/wine-prediction')\n",
    "print(f'train path: {train_path}')\n",
    "print(f'test path: {test_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cbb6511-d907-4329-a10b-ea8848f74447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.52</td>\n",
       "      <td>2.43</td>\n",
       "      <td>2.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.22</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>2.78</td>\n",
       "      <td>325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.73</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.70</td>\n",
       "      <td>22.5</td>\n",
       "      <td>101</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.70</td>\n",
       "      <td>1.19</td>\n",
       "      <td>2.71</td>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.28</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.84</td>\n",
       "      <td>15.5</td>\n",
       "      <td>110</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.36</td>\n",
       "      <td>4.60</td>\n",
       "      <td>1.09</td>\n",
       "      <td>2.78</td>\n",
       "      <td>880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.75</td>\n",
       "      <td>1.73</td>\n",
       "      <td>2.41</td>\n",
       "      <td>16.0</td>\n",
       "      <td>89</td>\n",
       "      <td>2.60</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.81</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.15</td>\n",
       "      <td>2.90</td>\n",
       "      <td>1320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.34</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.70</td>\n",
       "      <td>25.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.31</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2.70</td>\n",
       "      <td>13.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.96</td>\n",
       "      <td>660</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>12.08</td>\n",
       "      <td>1.83</td>\n",
       "      <td>2.32</td>\n",
       "      <td>18.5</td>\n",
       "      <td>81</td>\n",
       "      <td>1.60</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.08</td>\n",
       "      <td>2.27</td>\n",
       "      <td>480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>14.16</td>\n",
       "      <td>2.51</td>\n",
       "      <td>2.48</td>\n",
       "      <td>20.0</td>\n",
       "      <td>91</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.24</td>\n",
       "      <td>9.70</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1.71</td>\n",
       "      <td>660</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>13.51</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.65</td>\n",
       "      <td>19.0</td>\n",
       "      <td>110</td>\n",
       "      <td>2.35</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.54</td>\n",
       "      <td>4.20</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>13.69</td>\n",
       "      <td>3.26</td>\n",
       "      <td>2.54</td>\n",
       "      <td>20.0</td>\n",
       "      <td>107</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.80</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.82</td>\n",
       "      <td>680</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>12.82</td>\n",
       "      <td>3.37</td>\n",
       "      <td>2.30</td>\n",
       "      <td>19.5</td>\n",
       "      <td>88</td>\n",
       "      <td>1.48</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.97</td>\n",
       "      <td>10.26</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.75</td>\n",
       "      <td>685</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>138 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0      12.52        2.43  2.17               21.0         88           2.55   \n",
       "1      13.73        1.50  2.70               22.5        101           3.00   \n",
       "2      13.28        1.64  2.84               15.5        110           2.60   \n",
       "3      13.75        1.73  2.41               16.0         89           2.60   \n",
       "4      14.34        1.68  2.70               25.0         98           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "133    12.08        1.83  2.32               18.5         81           1.60   \n",
       "134    14.16        2.51  2.48               20.0         91           1.68   \n",
       "135    13.51        1.80  2.65               19.0        110           2.35   \n",
       "136    13.69        3.26  2.54               20.0        107           1.83   \n",
       "137    12.82        3.37  2.30               19.5         88           1.48   \n",
       "\n",
       "     flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0          2.27                  0.26             1.22             2.00  0.90   \n",
       "1          3.25                  0.29             2.38             5.70  1.19   \n",
       "2          2.68                  0.34             1.36             4.60  1.09   \n",
       "3          2.76                  0.29             1.81             5.60  1.15   \n",
       "4          1.31                  0.53             2.70            13.00  0.57   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "133        1.50                  0.52             1.64             2.40  1.08   \n",
       "134        0.70                  0.44             1.24             9.70  0.62   \n",
       "135        2.53                  0.29             1.54             4.20  1.10   \n",
       "136        0.56                  0.50             0.80             5.88  0.96   \n",
       "137        0.66                  0.40             0.97            10.26  0.72   \n",
       "\n",
       "     od280/od315_of_diluted_wines  proline  target  \n",
       "0                            2.78      325       1  \n",
       "1                            2.71     1285       0  \n",
       "2                            2.78      880       0  \n",
       "3                            2.90     1320       0  \n",
       "4                            1.96      660       2  \n",
       "..                            ...      ...     ...  \n",
       "133                          2.27      480       1  \n",
       "134                          1.71      660       2  \n",
       "135                          2.87     1095       0  \n",
       "136                          1.82      680       2  \n",
       "137                          1.75      685       2  \n",
       "\n",
       "[138 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/wine-prediction/train_data_wine_pred.csv')\n",
    "train_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "train_df['target'] = train_df['target'] - 1\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b0c0a7e-be15-4a77-b67a-f268f1f60162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.10</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.40</td>\n",
       "      <td>18.8</td>\n",
       "      <td>103</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2.92</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.38</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.75</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>2.38</td>\n",
       "      <td>22.0</td>\n",
       "      <td>80</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.42</td>\n",
       "      <td>1.35</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.57</td>\n",
       "      <td>580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.38</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.38</td>\n",
       "      <td>12.0</td>\n",
       "      <td>102</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.29</td>\n",
       "      <td>2.96</td>\n",
       "      <td>7.50</td>\n",
       "      <td>1.20</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1547</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.07</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15.5</td>\n",
       "      <td>98</td>\n",
       "      <td>2.40</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>1.37</td>\n",
       "      <td>3.70</td>\n",
       "      <td>1.18</td>\n",
       "      <td>2.69</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.76</td>\n",
       "      <td>2.68</td>\n",
       "      <td>2.92</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2.03</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.50</td>\n",
       "      <td>607</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.87</td>\n",
       "      <td>1.90</td>\n",
       "      <td>2.80</td>\n",
       "      <td>19.4</td>\n",
       "      <td>107</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.97</td>\n",
       "      <td>0.37</td>\n",
       "      <td>1.76</td>\n",
       "      <td>4.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.40</td>\n",
       "      <td>915</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.06</td>\n",
       "      <td>1.63</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>126</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.10</td>\n",
       "      <td>5.65</td>\n",
       "      <td>1.09</td>\n",
       "      <td>3.71</td>\n",
       "      <td>780</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.62</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.28</td>\n",
       "      <td>18.0</td>\n",
       "      <td>98</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2.26</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.35</td>\n",
       "      <td>3.25</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.96</td>\n",
       "      <td>345</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12.42</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1.68</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.42</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.30</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.51</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2.25</td>\n",
       "      <td>17.5</td>\n",
       "      <td>85</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.51</td>\n",
       "      <td>650</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.29</td>\n",
       "      <td>2.83</td>\n",
       "      <td>2.22</td>\n",
       "      <td>18.0</td>\n",
       "      <td>88</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.99</td>\n",
       "      <td>2.15</td>\n",
       "      <td>1.15</td>\n",
       "      <td>3.30</td>\n",
       "      <td>290</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13.17</td>\n",
       "      <td>5.19</td>\n",
       "      <td>2.32</td>\n",
       "      <td>22.0</td>\n",
       "      <td>93</td>\n",
       "      <td>1.74</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.55</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.48</td>\n",
       "      <td>725</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.45</td>\n",
       "      <td>3.70</td>\n",
       "      <td>2.60</td>\n",
       "      <td>23.0</td>\n",
       "      <td>111</td>\n",
       "      <td>1.70</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.46</td>\n",
       "      <td>10.68</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.56</td>\n",
       "      <td>695</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12.81</td>\n",
       "      <td>2.31</td>\n",
       "      <td>2.40</td>\n",
       "      <td>24.0</td>\n",
       "      <td>98</td>\n",
       "      <td>1.15</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.83</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.36</td>\n",
       "      <td>560</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.88</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.59</td>\n",
       "      <td>15.0</td>\n",
       "      <td>101</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.70</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.88</td>\n",
       "      <td>3.56</td>\n",
       "      <td>1095</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12.33</td>\n",
       "      <td>1.10</td>\n",
       "      <td>2.28</td>\n",
       "      <td>16.0</td>\n",
       "      <td>101</td>\n",
       "      <td>2.05</td>\n",
       "      <td>1.09</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.41</td>\n",
       "      <td>3.27</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.67</td>\n",
       "      <td>680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12.22</td>\n",
       "      <td>1.29</td>\n",
       "      <td>1.94</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2.08</td>\n",
       "      <td>2.70</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.02</td>\n",
       "      <td>312</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>13.72</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.7</td>\n",
       "      <td>108</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2.04</td>\n",
       "      <td>6.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.87</td>\n",
       "      <td>1285</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.65</td>\n",
       "      <td>1.67</td>\n",
       "      <td>2.62</td>\n",
       "      <td>26.0</td>\n",
       "      <td>88</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.34</td>\n",
       "      <td>2.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>3.21</td>\n",
       "      <td>562</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13.83</td>\n",
       "      <td>1.65</td>\n",
       "      <td>2.60</td>\n",
       "      <td>17.2</td>\n",
       "      <td>94</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.60</td>\n",
       "      <td>1.24</td>\n",
       "      <td>3.37</td>\n",
       "      <td>1265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>13.78</td>\n",
       "      <td>2.76</td>\n",
       "      <td>2.30</td>\n",
       "      <td>22.0</td>\n",
       "      <td>90</td>\n",
       "      <td>1.35</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1.03</td>\n",
       "      <td>9.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.68</td>\n",
       "      <td>615</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12.08</td>\n",
       "      <td>2.08</td>\n",
       "      <td>1.70</td>\n",
       "      <td>17.5</td>\n",
       "      <td>97</td>\n",
       "      <td>2.23</td>\n",
       "      <td>2.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.40</td>\n",
       "      <td>3.30</td>\n",
       "      <td>1.27</td>\n",
       "      <td>2.96</td>\n",
       "      <td>710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>14.10</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.30</td>\n",
       "      <td>18.0</td>\n",
       "      <td>105</td>\n",
       "      <td>2.95</td>\n",
       "      <td>3.32</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.38</td>\n",
       "      <td>5.75</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1510</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>13.71</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2.36</td>\n",
       "      <td>16.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.61</td>\n",
       "      <td>2.88</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.69</td>\n",
       "      <td>3.80</td>\n",
       "      <td>1.11</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1035</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>12.17</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.53</td>\n",
       "      <td>19.0</td>\n",
       "      <td>104</td>\n",
       "      <td>1.89</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.45</td>\n",
       "      <td>1.03</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.45</td>\n",
       "      <td>2.23</td>\n",
       "      <td>355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11.56</td>\n",
       "      <td>2.05</td>\n",
       "      <td>3.23</td>\n",
       "      <td>28.5</td>\n",
       "      <td>119</td>\n",
       "      <td>3.18</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.47</td>\n",
       "      <td>1.87</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>3.69</td>\n",
       "      <td>465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14.19</td>\n",
       "      <td>1.59</td>\n",
       "      <td>2.48</td>\n",
       "      <td>16.5</td>\n",
       "      <td>108</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.86</td>\n",
       "      <td>8.70</td>\n",
       "      <td>1.23</td>\n",
       "      <td>2.82</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12.77</td>\n",
       "      <td>3.43</td>\n",
       "      <td>1.98</td>\n",
       "      <td>16.0</td>\n",
       "      <td>80</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.83</td>\n",
       "      <td>3.40</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.12</td>\n",
       "      <td>372</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12.34</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.46</td>\n",
       "      <td>21.0</td>\n",
       "      <td>98</td>\n",
       "      <td>2.56</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.31</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3.38</td>\n",
       "      <td>438</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0     14.10        2.02  2.40               18.8        103           2.75   \n",
       "1     12.04        4.30  2.38               22.0         80           2.10   \n",
       "2     14.38        1.87  2.38               12.0        102           3.30   \n",
       "3     13.07        1.50  2.10               15.5         98           2.40   \n",
       "4     11.76        2.68  2.92               20.0        103           1.75   \n",
       "5     13.87        1.90  2.80               19.4        107           2.95   \n",
       "6     14.06        1.63  2.28               16.0        126           3.00   \n",
       "7     11.62        1.99  2.28               18.0         98           3.02   \n",
       "8     12.42        2.55  2.27               22.0         90           1.68   \n",
       "9     12.51        1.24  2.25               17.5         85           2.00   \n",
       "10    12.29        2.83  2.22               18.0         88           2.45   \n",
       "11    13.17        5.19  2.32               22.0         93           1.74   \n",
       "12    13.45        3.70  2.60               23.0        111           1.70   \n",
       "13    12.81        2.31  2.40               24.0         98           1.15   \n",
       "14    13.88        1.89  2.59               15.0        101           3.25   \n",
       "15    12.33        1.10  2.28               16.0        101           2.05   \n",
       "16    12.22        1.29  1.94               19.0         92           2.36   \n",
       "17    13.72        1.43  2.50               16.7        108           3.40   \n",
       "18    11.65        1.67  2.62               26.0         88           1.92   \n",
       "19    13.83        1.65  2.60               17.2         94           2.45   \n",
       "20    13.78        2.76  2.30               22.0         90           1.35   \n",
       "21    12.08        2.08  1.70               17.5         97           2.23   \n",
       "22    14.10        2.16  2.30               18.0        105           2.95   \n",
       "23    13.71        1.86  2.36               16.6        101           2.61   \n",
       "24    12.17        1.45  2.53               19.0        104           1.89   \n",
       "25    11.56        2.05  3.23               28.5        119           3.18   \n",
       "26    14.19        1.59  2.48               16.5        108           3.30   \n",
       "27    13.71        5.65  2.45               20.5         95           1.68   \n",
       "28    12.77        3.43  1.98               16.0         80           1.63   \n",
       "29    12.34        2.45  2.46               21.0         98           2.56   \n",
       "\n",
       "    flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0         2.92                  0.32             2.38             6.20  1.07   \n",
       "1         1.75                  0.42             1.35             2.60  0.79   \n",
       "2         3.64                  0.29             2.96             7.50  1.20   \n",
       "3         2.64                  0.28             1.37             3.70  1.18   \n",
       "4         2.03                  0.60             1.05             3.80  1.23   \n",
       "5         2.97                  0.37             1.76             4.50  1.25   \n",
       "6         3.17                  0.24             2.10             5.65  1.09   \n",
       "7         2.26                  0.17             1.35             3.25  1.16   \n",
       "8         1.84                  0.66             1.42             2.70  0.86   \n",
       "9         0.58                  0.60             1.25             5.45  0.75   \n",
       "10        2.25                  0.25             1.99             2.15  1.15   \n",
       "11        0.63                  0.61             1.55             7.90  0.60   \n",
       "12        0.92                  0.43             1.46            10.68  0.85   \n",
       "13        1.09                  0.27             0.83             5.70  0.66   \n",
       "14        3.56                  0.17             1.70             5.43  0.88   \n",
       "15        1.09                  0.63             0.41             3.27  1.25   \n",
       "16        2.04                  0.39             2.08             2.70  0.86   \n",
       "17        3.67                  0.19             2.04             6.80  0.89   \n",
       "18        1.61                  0.40             1.34             2.60  1.36   \n",
       "19        2.99                  0.22             2.29             5.60  1.24   \n",
       "20        0.68                  0.41             1.03             9.58  0.70   \n",
       "21        2.17                  0.26             1.40             3.30  1.27   \n",
       "22        3.32                  0.22             2.38             5.75  1.25   \n",
       "23        2.88                  0.27             1.69             3.80  1.11   \n",
       "24        1.75                  0.45             1.03             2.95  1.45   \n",
       "25        5.08                  0.47             1.87             6.00  0.93   \n",
       "26        3.93                  0.32             1.86             8.70  1.23   \n",
       "27        0.61                  0.52             1.06             7.70  0.64   \n",
       "28        1.25                  0.43             0.83             3.40  0.70   \n",
       "29        2.11                  0.34             1.31             2.80  0.80   \n",
       "\n",
       "    od280/od315_of_diluted_wines  proline  target  \n",
       "0                           2.75     1060       0  \n",
       "1                           2.57      580       1  \n",
       "2                           3.00     1547       0  \n",
       "3                           2.69     1020       0  \n",
       "4                           2.50      607       1  \n",
       "5                           3.40      915       0  \n",
       "6                           3.71      780       0  \n",
       "7                           2.96      345       1  \n",
       "8                           3.30      315       1  \n",
       "9                           1.51      650       2  \n",
       "10                          3.30      290       1  \n",
       "11                          1.48      725       2  \n",
       "12                          1.56      695       2  \n",
       "13                          1.36      560       2  \n",
       "14                          3.56     1095       0  \n",
       "15                          1.67      680       1  \n",
       "16                          3.02      312       1  \n",
       "17                          2.87     1285       0  \n",
       "18                          3.21      562       1  \n",
       "19                          3.37     1265       0  \n",
       "20                          1.68      615       2  \n",
       "21                          2.96      710       1  \n",
       "22                          3.17     1510       0  \n",
       "23                          4.00     1035       0  \n",
       "24                          2.23      355       1  \n",
       "25                          3.69      465       1  \n",
       "26                          2.82     1680       0  \n",
       "27                          1.74      740       2  \n",
       "28                          2.12      372       1  \n",
       "29                          3.38      438       1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/wine-prediction/test_data_wine_pred.csv',nrows=30)\n",
    "test_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "test_df['target'] = test_df['target'] - 1\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad344b36-9881-4de4-aa9e-f0c73ea2d71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549ee676b11147f6a9b68c31a25a4c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5c824772f948ca8e01fba7fc056d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be8c09612434f1c8472d85826330d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df90be4818e14b18a1c5245f65e45b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(train_df)\n",
    "profile.to_file('profile_report_wine.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32d79ed7-7c98-407d-aeed-f9c69b9f085a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium',\n",
       "       'total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
       "       'proanthocyanins', 'color_intensity', 'hue',\n",
       "       'od280/od315_of_diluted_wines', 'proline', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "943ed939-1694-436e-bb1b-19c2a4f4cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         feature         VIF\n",
      "0                          const  656.142518\n",
      "1                        alcohol    2.441252\n",
      "2                     malic_acid    1.607770\n",
      "3                            ash    2.114522\n",
      "4              alcalinity_of_ash    2.213630\n",
      "5                      magnesium    1.519026\n",
      "6                  total_phenols    4.347040\n",
      "7                     flavanoids    7.697181\n",
      "8           nonflavanoid_phenols    1.820228\n",
      "9                proanthocyanins    2.091542\n",
      "10               color_intensity    3.093786\n",
      "11                           hue    2.750493\n",
      "12  od280/od315_of_diluted_wines    3.879269\n",
      "13                       proline    2.739025\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Assuming 'data' contains your features\n",
    "# Extract the feature columns\n",
    "X = train_df.drop(columns=['target'])\n",
    "\n",
    "# Add a constant column to the features (required for VIF calculation)\n",
    "X_with_const = pd.concat([pd.Series(1, index=X.index, name='const'), X], axis=1)\n",
    "\n",
    "# Create a DataFrame to store the VIF results\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_with_const.columns\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(X_with_const.shape[1])]\n",
    "\n",
    "# Print the VIF results\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4024d7eb-3bc5-4f24-82df-e9f54c896963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (138, 13) (138,)\n",
      "Test data shape: (30, 13) (30,)\n"
     ]
    }
   ],
   "source": [
    "#split the data\n",
    "X_train = train_df.drop(['target'], axis=1)  # drop target and unimportant features\n",
    "y_train = train_df['target']  # Target variable\n",
    "X_test = test_df.drop(['target'], axis=1)  # Features\n",
    "y_test = test_df['target']  # Target variable\n",
    "print(\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e6227be-6a26-4856-8409-dbab8bee02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the numerical and categorical columns\n",
    "import numpy as np\n",
    "def data_type(dataset):\n",
    "    \"\"\"\n",
    "    Function to identify the numerical and categorical data columns\n",
    "    :param dataset: Dataframe\n",
    "    :return: list of numerical and categorical columns\n",
    "    \"\"\"\n",
    "    numerical = []\n",
    "    categorical = []\n",
    "    for i in dataset.columns:\n",
    "        if dataset[i].dtype == 'int64' or dataset[i].dtype == 'float64':\n",
    "            numerical.append(i)\n",
    "        else:\n",
    "            categorical.append(i)\n",
    "    return numerical, categorical\n",
    "\n",
    "\n",
    "numerical, categorical = data_type(X_train)\n",
    "\n",
    "# Identifying the binary columns and ignoring them from scaling\n",
    "def binary_columns(df):\n",
    "    \"\"\"\n",
    "    Generates a list of binary columns in a dataframe.\n",
    "    \"\"\"\n",
    "    binary_cols = []\n",
    "    for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "        unique_values = df[col].unique()\n",
    "        if np.in1d(unique_values, [0, 1]).all():\n",
    "            binary_cols.append(col)\n",
    "    return binary_cols\n",
    "\n",
    "binary_cols = binary_columns(X_train)\n",
    "\n",
    "# Remove the binary columns from the numerical columns\n",
    "categorical = [i for i in categorical if i != 'target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6384ed90-ced1-4878-80b1-07bdae11fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Create a pipeline for preprocessing\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical),  # StandardScaler for numeric features\n",
    "        ('cat', CatBoostEncoder(), categorical)  # CatBoostEncoder for categorical features\n",
    "    ],\n",
    "    remainder='passthrough'  # Passthrough any columns not specified\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b812ef55-556a-4f5d-803e-dbfa97146d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define classifiers\n",
    "rfc = RandomForestClassifier()\n",
    "logistic = LogisticRegression(penalty='l2', max_iter=1000)\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "xgb = XGBClassifier()\n",
    "nn = MLPClassifier()\n",
    "\n",
    "# Create pipelines for classifiers with the updated ColumnTransformer\n",
    "rfc_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"Random Forest\", rfc)\n",
    "])\n",
    "\n",
    "logistic_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"Logistic Regression\", logistic)\n",
    "])\n",
    "\n",
    "svm_rbf_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"SVM with RBF kernel\", svm_rbf)\n",
    "])\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"XGBoost\", xgb)\n",
    "])\n",
    "\n",
    "nn_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"Neural Network\", nn)\n",
    "])\n",
    "\n",
    "# Fit and use the pipelines as before\n",
    "rfc_pipeline.fit(X_train, y_train)\n",
    "logistic_pipeline.fit(X_train, y_train)\n",
    "svm_rbf_pipeline.fit(X_train, y_train)\n",
    "xgb_pipeline.fit(X_train, y_train)\n",
    "nn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# After fitting, you can use the pipelines for prediction and evaluation\n",
    "rfc_predictions = rfc_pipeline.predict(X_test)\n",
    "logistic_predictions = logistic_pipeline.predict(X_test)\n",
    "svm_rbf_predictions = svm_rbf_pipeline.predict(X_test)\n",
    "xgb_predictions = xgb_pipeline.predict(X_test)\n",
    "nn_predictions = nn_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fca71359-dfc7-4abc-8e6b-ae52939731a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display =\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfc30267-91c6-4b5b-87d2-d1c1844599af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XG boost Training Accuracy:1.0000\n",
      "XG boost Test Accuracy:1.0000\n"
     ]
    }
   ],
   "source": [
    "xgb_train_accuracy= xgb_pipeline.score(X_train, y_train)\n",
    "print(f\"XG boost Training Accuracy:{xgb_train_accuracy:.4f}\")\n",
    "xgb_test_accuracy= xgb_pipeline.score(X_test, y_test)\n",
    "print(f\"XG boost Test Accuracy:{xgb_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7164016f-95d3-4361-bfbf-de4f3f4404a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy:1.0000\n",
      "Random Forest Test Accuracy:1.0000\n"
     ]
    }
   ],
   "source": [
    "rfc_train_accuracy= rfc_pipeline.score(X_train, y_train)\n",
    "print(f\"Random Forest Training Accuracy:{rfc_train_accuracy:.4f}\")\n",
    "rfc_test_accuracy= rfc_pipeline.score(X_test, y_test)\n",
    "print(f\"Random Forest Test Accuracy:{rfc_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fbc29f3-fdca-4935-908d-08dd0a661ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Ridge Training Accuracy:1.0000\n",
      "Logistic Ridge Test Accuracy:1.0000\n"
     ]
    }
   ],
   "source": [
    "logistic_train_accuracy= logistic_pipeline.score(X_train, y_train)\n",
    "print(f\"Logistic Ridge Training Accuracy:{logistic_train_accuracy:.4f}\")\n",
    "logistic_test_accuracy= logistic_pipeline.score(X_test, y_test)\n",
    "print(f\"Logistic Ridge Test Accuracy:{logistic_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90670332-6739-4daa-bc58-6b75ef567cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy:0.9928\n",
      "SVM Test Accuracy:1.0000\n"
     ]
    }
   ],
   "source": [
    "svm_train_accuracy= svm_rbf_pipeline.score(X_train, y_train)\n",
    "print(f\"SVM Training Accuracy:{svm_train_accuracy:.4f}\")\n",
    "svm_test_accuracy= svm_rbf_pipeline.score(X_test, y_test)\n",
    "print(f\"SVM Test Accuracy:{svm_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1473b07-3f49-43fa-b41f-c0955080fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Training Accuracy:1.0000\n",
      "Neural Network Test Accuracy:0.9667\n"
     ]
    }
   ],
   "source": [
    "nn_train_accuracy= nn_pipeline.score(X_train, y_train)\n",
    "print(f\"Neural Network Training Accuracy:{nn_train_accuracy:.4f}\")\n",
    "nn_test_accuracy= nn_pipeline.score(X_test, y_test)\n",
    "print(f\"Neural Network Test Accuracy:{nn_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1caa549e-bdcf-490a-a0a5-4f658cbd67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "import ast\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--rfc_n_estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--rfc_min_samples_split\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--rfc_criterion\", type=str, default=\"gini\")\n",
    "    parser.add_argument(\"--logistic_max_iter\", type=int, default=500)\n",
    "    parser.add_argument(\"--svm_kernel\", type=str, default=\"rbf\")\n",
    "    parser.add_argument(\"--xgb_max_depth\", type=int, default=3)\n",
    "    parser.add_argument(\"--nn_hidden_layer_sizes\", type=str, default=\"(100,)\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # Read the data\n",
    "    train_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/wine-prediction/train_data_wine_pred.csv')  # Path to your train data file\n",
    "    test_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/wine-prediction/test_data_wine_pred.csv',nrows=50)   # Path to your test data file\n",
    "    \n",
    "    # Convert target variable to numerical labels starting from 0\n",
    "    label_encoder = LabelEncoder()\n",
    "    train_df['target'] = label_encoder.fit_transform(train_df['target'])\n",
    "    test_df['target'] = label_encoder.transform(test_df['target'])\n",
    "    \n",
    "    X_train = train_df.drop(\"target\", axis=1)\n",
    "    y_train = train_df[\"target\"]\n",
    "    X_test = test_df.drop(\"target\", axis=1)\n",
    "    y_test = test_df[\"target\"] \n",
    "    \n",
    "    def data_type(dataset):\n",
    "        \"\"\"\n",
    "        Function to identify the numerical and categorical data columns\n",
    "        :param dataset: Dataframe\n",
    "        :return: list of numerical and categorical columns\n",
    "        \"\"\"\n",
    "        numerical = []\n",
    "        categorical = []\n",
    "        for i in dataset.columns:\n",
    "            if dataset[i].dtype == 'int64' or dataset[i].dtype == 'float64':\n",
    "                numerical.append(i)\n",
    "            else:\n",
    "                categorical.append(i)\n",
    "        return numerical, categorical\n",
    "\n",
    "    numerical, categorical = data_type(X_train)\n",
    "    \n",
    "    # Identifying the binary columns and ignoring them from scaling\n",
    "    def binary_columns(df):\n",
    "        \"\"\"\n",
    "        Generates a list of binary columns in a dataframe.\n",
    "        \"\"\"\n",
    "        binary_cols = []\n",
    "        for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "            unique_values = df[col].unique()\n",
    "            if np.in1d(unique_values, [0, 1]).all():\n",
    "                binary_cols.append(col)\n",
    "        return binary_cols\n",
    "\n",
    "    binary_cols = binary_columns(X_train)\n",
    "\n",
    "    # Remove the binary columns from the numerical columns\n",
    "    numerical = [i for i in numerical if i not in binary_cols]\n",
    "    \n",
    "    # Define your encoder\n",
    "    ct = ColumnTransformer([\n",
    "        (\"CatBoostEncoding\", CatBoostEncoder(), categorical),\n",
    "        (\"Scaling\", StandardScaler(), numerical)\n",
    "    ])\n",
    "    \n",
    "    # Define classifiers\n",
    "    rfc = RandomForestClassifier(n_estimators=args.rfc_n_estimators, \n",
    "                                  min_samples_split=args.rfc_min_samples_split, \n",
    "                                  criterion=args.rfc_criterion)\n",
    "    \n",
    "    logistic = LogisticRegression(penalty='l2', max_iter=args.logistic_max_iter)\n",
    "    \n",
    "    svm_rbf = SVC(kernel=args.svm_kernel)\n",
    "    \n",
    "    xgb_classifier = xgb.XGBClassifier(max_depth=args.xgb_max_depth)\n",
    "\n",
    "    # Convert hidden_layer_sizes from string to tuple of integers\n",
    "    hidden_layer_sizes = ast.literal_eval(args.nn_hidden_layer_sizes)\n",
    "    \n",
    "    nn_classifier = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes)\n",
    "    \n",
    "    # Create pipelines for classifiers with your encoder\n",
    "    rfc_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"Random Forest\", rfc)\n",
    "    ])\n",
    "\n",
    "    logistic_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"Logistic Regression\", logistic)\n",
    "    ])\n",
    "\n",
    "    svm_rbf_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"SVM with RBF kernel\", svm_rbf)\n",
    "    ])\n",
    "    \n",
    "    xgb_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"XGBoost Classifier\", xgb_classifier)\n",
    "    ])\n",
    "    \n",
    "    nn_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"Neural Network Classifier\", nn_classifier)\n",
    "    ])\n",
    "    \n",
    "    # Fit and evaluate each pipeline\n",
    "    for pipeline, name in [(rfc_pipeline, 'Random Forest'), \n",
    "                           (logistic_pipeline, 'Logistic Regression'), \n",
    "                           (svm_rbf_pipeline, 'SVM with RBF kernel'),\n",
    "                           (xgb_pipeline, 'XGBoost Classifier'),\n",
    "                           (nn_pipeline, 'Neural Network Classifier')]:\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        train_accuracy = pipeline.score(X_train, y_train)\n",
    "        test_accuracy = pipeline.score(X_test, y_test)\n",
    "        print(f\"{name} Training Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        # Save the model\n",
    "        model_save_path = os.path.join(args.model_dir, f\"{name.lower().replace(' ', '_')}_model.joblib\")\n",
    "        joblib.dump(pipeline, model_save_path)\n",
    "        print(f\"Model Saved At: {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f46222c2-e46c-47d0-82bb-4b33425b8482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "xgboost\n",
    "scikit-learn\n",
    "fsspec\n",
    "category_encoders\n",
    "s3fs\n",
    "botocore==1.27.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad53323c-8620-4ec9-8764-42d26c78b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy: 1.0000\n",
      "Model Saved At: ../models/random_forest_model.joblib\n",
      "Logistic Regression Training Accuracy: 1.0000\n",
      "Model Saved At: ../models/logistic_regression_model.joblib\n",
      "SVM with RBF kernel Training Accuracy: 0.9750\n",
      "Model Saved At: ../models/svm_with_rbf_kernel_model.joblib\n",
      "XGBoost Classifier Training Accuracy: 0.9750\n",
      "Model Saved At: ../models/xgboost_classifier_model.joblib\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "Neural Network Classifier Training Accuracy: 0.9500\n",
      "Model Saved At: ../models/neural_network_classifier_model.joblib\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model_dir ../models/ --rfc_n_estimators 100 --rfc_min_samples_split 0.05 --rfc_criterion gini --logistic_max_iter 1000 --svm_kernel rbf --xgb_max_depth 3 --nn_hidden_layer_sizes !python train.py --model_dir ../models/ --rfc_n_estimators 100 --rfc_min_samples_split 0.05 --rfc_criterion gini --logistic_max_iter 1000 --svm_kernel rbf --xgb_max_depth 3 --nn_hidden_layer_sizes \"(25,11,75,5,3,100)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8121103-4087-4429-8c10-25cdb71343c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: pipeline-run-2024-03-03-14-17-27-149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2024-03-03 14:17:27 Starting - Starting the training job...\n",
      "2024-03-03 14:17:42 Starting - Preparing the instances for training...\n",
      "2024-03-03 14:18:20 Downloading - Downloading input data...\n",
      "2024-03-03 14:18:41 Downloading - Downloading the training image...\n",
      "2024-03-03 14:19:21 Training - Training image download completed. Training in progress..\u001b[34m2024-03-03 14:19:24,509 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:24,512 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:24,547 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:24,722 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /miniconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.1.3)\u001b[0m\n",
      "\u001b[34mCollecting xgboost\n",
      "  Downloading xgboost-1.6.2-py3-none-manylinux2014_x86_64.whl (255.9 MB)\u001b[0m\n",
      "\u001b[34m     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 255.9/255.9 MB 5.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /miniconda3/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (0.23.2)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 kB 34.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting category_encoders\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.9/81.9 kB 18.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2023.1.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore==1.27.18 in /miniconda3/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.27.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore==1.27.18->-r requirements.txt (line 7)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.7/site-packages (from botocore==1.27.18->-r requirements.txt (line 7)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from botocore==1.27.18->-r requirements.txt (line 7)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /miniconda3/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15.4 in /miniconda3/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (1.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /miniconda3/lib/python3.7/site-packages (from xgboost->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /miniconda3/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /miniconda3/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels>=0.9.0\n",
      "  Downloading statsmodels-0.13.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 93.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.9/233.9 kB 51.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (987 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 988.0/988.0 kB 89.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.4.2\n",
      "  Downloading aiobotocore-2.4.2-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 1.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.5/77.5 kB 22.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.11.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.5/139.5 kB 30.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.4.0\n",
      "  Downloading aiobotocore-2.4.1-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 18.0 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.8/65.8 kB 19.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.10.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 26.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.8.2-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.8/140.8 kB 27.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.8.1-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.8.1-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.8/140.8 kB 20.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.8.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.8.0-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.0/141.0 kB 21.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.7.1-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.2/141.2 kB 26.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.3.4\n",
      "  Downloading aiobotocore-2.3.4-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.7.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.2/141.2 kB 35.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.5.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 29.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.3.0\n",
      "  Downloading aiobotocore-2.3.3.tar.gz (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 kB 20.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.3.2.tar.gz (104 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.8/104.8 kB 20.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.3.1.tar.gz (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.3/65.3 kB 1.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.3.0.tar.gz (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.1/65.1 kB 4.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.3.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.2.0\n",
      "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.7/59.7 kB 7.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.1/136.1 kB 3.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.2.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.2-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.0/56.0 kB 2.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.9/134.9 kB 4.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 16.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 kB 14.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.2/133.2 kB 32.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.11.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.0/133.0 kB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.0.1\n",
      "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 15.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.4/132.4 kB 26.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.5/52.5 kB 15.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.3/52.3 kB 15.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.10.1-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.6/125.6 kB 36.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.10.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.0/125.0 kB 34.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.9.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.6/123.6 kB 35.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.3/119.3 kB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=1.4.0\n",
      "  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.6/51.6 kB 15.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.8.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.1/118.1 kB 31.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-2.6.0-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 16.5 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 19.4 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.3-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 21.8 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.2-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 22.2 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.1-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.8/72.8 kB 21.7 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.0-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 23.7 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 14.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.6/50.6 kB 11.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 kB 8.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 10.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.2/48.2 kB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.1/48.1 kB 9.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m  Downloading aiobotocore-1.2.1.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.0/48.0 kB 10.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.2.0.tar.gz (47 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.3/47.3 kB 9.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.1.2-py3-none-any.whl (45 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 9.6 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.1.1-py3-none-any.whl (45 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.0/45.0 kB 12.8 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.1.0-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 13.9 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.7-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.9/42.9 kB 13.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m  Downloading aiobotocore-1.0.6-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 kB 12.7 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.5-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 kB 14.2 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.4-py3-none-any.whl (41 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.6/41.6 kB 12.4 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 11.8 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.2-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 12.5 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.1-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.7/40.7 kB 12.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.6.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.6.1-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.1/115.1 kB 27.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.6.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.6.0-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.7/114.7 kB 25.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.5.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.5.0-py3-none-any.whl (111 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.7/111.7 kB 26.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.4.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.4.0-py3-none-any.whl (108 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.3/108.3 kB 30.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-0.6.0-py3-none-any.whl (23 kB)\n",
      "  Downloading s3fs-0.5.2-py3-none-any.whl (22 kB)\n",
      "  Downloading s3fs-0.5.1-py3-none-any.whl (21 kB)\n",
      "  Downloading s3fs-0.5.0-py3-none-any.whl (21 kB)\n",
      "  Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.7/site-packages (from patsy>=0.5.1->category_encoders->-r requirements.txt (line 5)) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting packaging>=21.3\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=3.1.0 in /miniconda3/lib/python3.7/site-packages (from importlib-resources->category_encoders->-r requirements.txt (line 5)) (3.13.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: patsy, packaging, importlib-resources, fsspec, xgboost, statsmodels, s3fs, category_encoders\u001b[0m\n",
      "\n",
      "2024-03-03 14:19:56 Uploading - Uploading generated training model\u001b[34mSuccessfully installed category_encoders-2.6.3 fsspec-2023.1.0 importlib-resources-5.12.0 packaging-23.2 patsy-0.5.6 s3fs-0.4.2 statsmodels-0.13.5 xgboost-1.6.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:47,324 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:47,336 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:47,348 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:47,356 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"logistic_max_iter\": 200,\n",
      "        \"nn_hidden_layer_sizes\": \"(25,11,75,5,3,100)\",\n",
      "        \"rfc_criterion\": \"gini\",\n",
      "        \"rfc_min_samples_split\": 0.05,\n",
      "        \"rfc_n_estimators\": 100,\n",
      "        \"svm_kernel\": \"rbf\",\n",
      "        \"xgb_max_depth\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pipeline-run-2024-03-03-14-17-27-149\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-14-17-27-149/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"logistic_max_iter\":200,\"nn_hidden_layer_sizes\":\"(25,11,75,5,3,100)\",\"rfc_criterion\":\"gini\",\"rfc_min_samples_split\":0.05,\"rfc_n_estimators\":100,\"svm_kernel\":\"rbf\",\"xgb_max_depth\":3}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-14-17-27-149/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"logistic_max_iter\":200,\"nn_hidden_layer_sizes\":\"(25,11,75,5,3,100)\",\"rfc_criterion\":\"gini\",\"rfc_min_samples_split\":0.05,\"rfc_n_estimators\":100,\"svm_kernel\":\"rbf\",\"xgb_max_depth\":3},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pipeline-run-2024-03-03-14-17-27-149\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-14-17-27-149/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--logistic_max_iter\",\"200\",\"--nn_hidden_layer_sizes\",\"(25,11,75,5,3,100)\",\"--rfc_criterion\",\"gini\",\"--rfc_min_samples_split\",\"0.05\",\"--rfc_n_estimators\",\"100\",\"--svm_kernel\",\"rbf\",\"--xgb_max_depth\",\"3\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_LOGISTIC_MAX_ITER=200\u001b[0m\n",
      "\u001b[34mSM_HP_NN_HIDDEN_LAYER_SIZES=(25,11,75,5,3,100)\u001b[0m\n",
      "\u001b[34mSM_HP_RFC_CRITERION=gini\u001b[0m\n",
      "\u001b[34mSM_HP_RFC_MIN_SAMPLES_SPLIT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_RFC_N_ESTIMATORS=100\u001b[0m\n",
      "\u001b[34mSM_HP_SVM_KERNEL=rbf\u001b[0m\n",
      "\u001b[34mSM_HP_XGB_MAX_DEPTH=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python train.py --logistic_max_iter 200 --nn_hidden_layer_sizes (25,11,75,5,3,100) --rfc_criterion gini --rfc_min_samples_split 0.05 --rfc_n_estimators 100 --svm_kernel rbf --xgb_max_depth 3\u001b[0m\n",
      "\u001b[34mRandom Forest Training Accuracy: 1.0000\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/random_forest_model.joblib\u001b[0m\n",
      "\u001b[34mLogistic Regression Training Accuracy: 1.0000\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/logistic_regression_model.joblib\u001b[0m\n",
      "\u001b[34mSVM with RBF kernel Training Accuracy: 0.9750\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/svm_with_rbf_kernel_model.joblib\u001b[0m\n",
      "\u001b[34mXGBoost Classifier Training Accuracy: 0.9750\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/xgboost_classifier_model.joblib\u001b[0m\n",
      "\u001b[34mNeural Network Classifier Training Accuracy: 0.9750\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/neural_network_classifier_model.joblib\u001b[0m\n",
      "\u001b[34m2024-03-03 14:19:49,482 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-03-03 14:20:07 Completed - Training job completed\n",
      "Training seconds: 107\n",
      "Billable seconds: 43\n",
      "Managed Spot Training savings: 59.8%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    base_job_name=\"pipeline-run\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    entry_point=\"train.py\",\n",
    "    dependencies=[\"requirements.txt\"],  # Include the requirements file if needed\n",
    "    hyperparameters={\n",
    "        \"rfc_n_estimators\": 100,\n",
    "        \"rfc_min_samples_split\": 0.05,\n",
    "        \"rfc_criterion\": \"gini\",\n",
    "        \"logistic_max_iter\": 200,\n",
    "        \"svm_kernel\": \"rbf\",\n",
    "        \"xgb_max_depth\": 3,\n",
    "        \"nn_hidden_layer_sizes\": \"(25,11,75,5,3,100)\"  # Pass as a string representing a tuple\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    use_spot_instances=True,\n",
    "    max_wait=600,\n",
    "    max_run=600,\n",
    "    role=get_execution_role(),\n",
    ")\n",
    "\n",
    "sklearn_estimator.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e359895-3714-4c52-9d02-3a057e4b43dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name : pipeline-run-2024-03-03-14-17-27-149\n",
      "Model storage location : s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-14-17-27-149/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "training_job_name = sklearn_estimator.latest_training_job.name\n",
    "model_artifact = sm_client.describe_training_job(\n",
    "    TrainingJobName = training_job_name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(f\"Training job name : {training_job_name}\")\n",
    "print(f\"Model storage location : {model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3fe6cf03-e00c-4494-b8f5-a5a2338e4d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker:Creating hyperparameter tuning job with name: pipeline-run-240303-1434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "............................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "import itertools\n",
    "\n",
    "# Define role\n",
    "role = sagemaker.get_execution_role()\n",
    "# Define the SKLearn estimator with the modified train.py script\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"train.py\",  # Make sure this points to the modified train.py script\n",
    "    source_dir=\".\",  # Directory containing your training script and dependencies\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    framework_version=\"0.23-1\"\n",
    ")\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"rfc_n_estimators\": IntegerParameter(50, 150),\n",
    "    \"rfc_min_samples_split\": ContinuousParameter(0.01, 0.5),\n",
    "    \"rfc_criterion\": CategoricalParameter([\"gini\", \"entropy\"]),\n",
    "    \"logistic_max_iter\": IntegerParameter(100, 1000),\n",
    "    \"svm_kernel\": CategoricalParameter([\"linear\", \"poly\", \"rbf\", \"sigmoid\"]),\n",
    "    \"xgb_max_depth\": IntegerParameter(3, 10),\n",
    "    \"nn_hidden_layer_sizes\": CategoricalParameter([(25,50,75,100),(10,20,30,40,50),(60,70,80,90,100)])\n",
    "}\n",
    "\n",
    "# Define the objective metric name and type\n",
    "objective_metric_name = 'Training_Accuracy'\n",
    "objective_type = 'Maximize'\n",
    "\n",
    "# Define the metric definitions function\n",
    "def generate_metric_definitions():\n",
    "    return [\n",
    "        {\n",
    "            \"Name\": \"Training_Accuracy\",\n",
    "            \"Regex\": \"[a-zA-Z].*\\\\s+Training\\\\s+Accuracy:\\\\s+([0-9\\\\.]+)\"\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"Test_Accuracy\",\n",
    "            \"Regex\": \"[a-zA-Z].*\\\\s+Test\\\\s+Accuracy:\\\\s+([0-9\\\\.]+)\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# Define the HyperparameterTuner\n",
    "tuner = HyperparameterTuner(\n",
    "    base_tuning_job_name='pipeline-run',\n",
    "    estimator=sklearn_estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    objective_type=objective_type,\n",
    "    metric_definitions=generate_metric_definitions(),\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2\n",
    ")\n",
    "\n",
    "# Launch the hyperparameter tuning job\n",
    "tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9c0baea-d703-4e48-8273-8bb43a5cbbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline-run-240303-1434'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_job_name = tuner.latest_tuning_job.name\n",
    "tuning_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "061c513d-0768-4228-8e85-9ef86bc75dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-03-03 14:37:52 Starting - Preparing the instances for training\n",
      "2024-03-03 14:37:52 Downloading - Downloading the training image\n",
      "2024-03-03 14:37:52 Training - Training image download completed. Training in progress.\n",
      "2024-03-03 14:37:52 Uploading - Uploading generated training model\n",
      "2024-03-03 14:37:52 Completed - Resource reused by training job: pipeline-run-240303-1434-004-d9bf51bc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_tuning_objective_metric': '\"Training_Accuracy\"',\n",
       " 'logistic_max_iter': '944',\n",
       " 'nn_hidden_layer_sizes': '\"(10, 20, 30, 40, 50)\"',\n",
       " 'rfc_criterion': '\"entropy\"',\n",
       " 'rfc_min_samples_split': '0.23860113313287065',\n",
       " 'rfc_n_estimators': '108',\n",
       " 'sagemaker_container_log_level': '20',\n",
       " 'sagemaker_estimator_class_name': '\"SKLearn\"',\n",
       " 'sagemaker_estimator_module': '\"sagemaker.sklearn.estimator\"',\n",
       " 'sagemaker_job_name': '\"sagemaker-scikit-learn-2024-03-03-14-34-22-586\"',\n",
       " 'sagemaker_program': '\"train.py\"',\n",
       " 'sagemaker_region': '\"us-east-1\"',\n",
       " 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-635439539142/sagemaker-scikit-learn-2024-03-03-14-34-22-586/source/sourcedir.tar.gz\"',\n",
       " 'svm_kernel': '\"poly\"',\n",
       " 'xgb_max_depth': '10'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters = tuner.best_estimator().hyperparameters()\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c27c5f1a-855c-4d6c-9a21-504a52a83297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "tuner_analytics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuning_metrics = tuner_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "200de4e9-4d8e-4fed-b32c-090c3a609bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the style for the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot hyperparameters versus objective metric using Seaborn\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (hyperparameter, ax) in enumerate(zip(hyperparameters.columns, axes)):\n",
    "    sns.scatterplot(data=tuning_metrics, x=hyperparameter, y='FinalObjectiveValue', ax=ax, alpha=0.5)\n",
    "    ax.set_xlabel(hyperparameter)\n",
    "    ax.set_ylabel('Final Objective Value')\n",
    "    ax.set_title(f'{hyperparameter} vs. Final Objective Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270bcbe-cade-40e4-9449-5b685b513ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
