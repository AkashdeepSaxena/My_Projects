{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary Prediction\n",
    "- [The Adult Salary Prediction dataset](https://archive.ics.uci.edu/ml/datasets/adult) consists of data from the 1994 US Census and the task is to predict whether a person earns `over $50K` a year (Class 1) or `less than $50K` a year (Class 0). The columns in the dataset are as follows:\n",
    "\n",
    "|col name|description|\n",
    "|:--|:--|\n",
    "|age| continuous.|\n",
    "|workclass| Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.|\n",
    "|fnlwgt| continuous.|\n",
    "|education| Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.|\n",
    "|education-num| continuous.|\n",
    "|marital-status| Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.|\n",
    "|occupation| Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.|\n",
    "|relationship| Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.|\n",
    "|race| White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.|\n",
    "|sex| Female, Male.|\n",
    "|capital-gain| continuous.|\n",
    "|capital-loss| continuous.|\n",
    "|hours-per-week| continuous.|\n",
    "|native-country| United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.|\n",
    "|target| This is the target variable to be predicted. Class 1 for salary >50K and class 0 for salary <=50K|\n",
    "\n",
    "- The goal of this project is to build and tune a model to predict the `target` column using AWS Sagemaker and deploy the model as a `Serverless Inference Endpoint`\n",
    "\n",
    "## Tips: \n",
    "- You can use the below code to get the S3 bucket to write any artifacts to\n",
    "    ```\n",
    "    import sagemaker\n",
    "    session = sagemaker.Session()\n",
    "    bucket = session.default_bucket()\n",
    "    ```\n",
    "- Are all the columns necessary or we can drop any?\n",
    "- What ML task is this? Classification? Regression? Clustering?\n",
    "- How to determine the best hyperparameters for the model?\n",
    "- How to test if the model is deployed successfully?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "cols = [\n",
    "    \"age\", \n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education-num\",\n",
    "    \"marital-status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital-gain\",\n",
    "    \"capital-loss\",\n",
    "    \"hours-per-week\",\n",
    "    \"native-country\",\n",
    "    \"salary\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15) (16281, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age          workclass  fnlwgt   education  education-num  \\\n",
       "0   39          State-gov   77516   Bachelors             13   \n",
       "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
       "2   38            Private  215646     HS-grad              9   \n",
       "3   53            Private  234721        11th              7   \n",
       "4   28            Private  338409   Bachelors             13   \n",
       "\n",
       "        marital-status          occupation    relationship    race      sex  \\\n",
       "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
       "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
       "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
       "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
       "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
       "0          2174             0              40   United-States       0  \n",
       "1             0             0              13   United-States       0  \n",
       "2             0             0              40   United-States       0  \n",
       "3             0             0              40   United-States       0  \n",
       "4             0             0              40            Cuba       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", names=cols)\n",
    "big_test_df = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", names=cols, skiprows=1)\n",
    "train_df[\"salary\"] = train_df[\"salary\"].apply(lambda x: 1 if \">50K\" in x else 0)\n",
    "big_test_df[\"salary\"] = big_test_df[\"salary\"].apply(lambda x: 1 if \">50K\" in x else 0)\n",
    "train_df.to_csv('../data/train_data_salary_pred.csv',index=False);\n",
    "big_test_df.to_csv('../data/test_data_salary_pred.csv',index=False);\n",
    "print(train_df.shape, big_test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the best!\n",
    "Get started below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32561, 15)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker-us-east-1-635439539142\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_test_df.to_json('../data/test_data_salary_pred_json.json',orient='records',lines=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train path: s3://sagemaker-us-east-1-635439539142/akash/salary-prediction/train_data_salary_pred.csv\n",
      "test path: s3://sagemaker-us-east-1-635439539142/akash/salary-prediction/test_data_salary_pred.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = session.upload_data(path='../data/train_data_salary_pred.csv', bucket=bucket, key_prefix = 'akash/salary-prediction')\n",
    "test_path = session.upload_data(path='../data/test_data_salary_pred.csv', bucket=bucket, key_prefix = 'akash/salary-prediction')\n",
    "print(f'train path: {train_path}')\n",
    "print(f'test path: {test_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/salary-prediction/train_data_salary_pred.csv',nrows=1000)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/salary-prediction/test_data_salary_pred.csv',nrows=300)\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3a9ec6ab074cfc9e29767d1593cdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89a5fbbc3eb46c5ab7d0d25645495ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb11858716ad44e6be99868c4883a2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c17fb674ade4deaaa3d77f41be11c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(train_df,correlations={\"auto\": {\"calculate\": False}})\n",
    "profile.to_file('profile_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the numerical and categorical columns\n",
    "\n",
    "def data_type(dataset):\n",
    "    \"\"\"\n",
    "    Function to identify the numerical and categorical data columns\n",
    "    :param dataset: Dataframe\n",
    "    :return: list of numerical and categorical columns\n",
    "    \"\"\"\n",
    "    numerical = []\n",
    "    categorical = []\n",
    "    for i in dataset.columns:\n",
    "        if dataset[i].dtype == 'int64' or dataset[i].dtype == 'float64':\n",
    "            numerical.append(i)\n",
    "        else:\n",
    "            categorical.append(i)\n",
    "    return numerical, categorical\n",
    "\n",
    "\n",
    "numerical, categorical = data_type(train_df)\n",
    "\n",
    "# Identifying the binary columns and ignoring them from scaling\n",
    "def binary_columns(df):\n",
    "    \"\"\"\n",
    "    Generates a list of binary columns in a dataframe.\n",
    "    \"\"\"\n",
    "    binary_cols = []\n",
    "    for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "        unique_values = df[col].unique()\n",
    "        if np.in1d(unique_values, [0, 1]).all():\n",
    "            binary_cols.append(col)\n",
    "    return binary_cols\n",
    "\n",
    "binary_cols = binary_columns(train_df)\n",
    "\n",
    "# Remove the binary columns from the numerical columns\n",
    "numerical = [i for i in numerical if i not in binary_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1000, 14) (1000,)\n",
      "Test data shape: (300, 14) (300,)\n"
     ]
    }
   ],
   "source": [
    "#split the data\n",
    "X_train = train_df.drop('salary', axis=1)  # Features\n",
    "y_train = train_df['salary']  # Target variable\n",
    "X_test = test_df.drop('salary', axis=1)  # Features\n",
    "y_test = test_df['salary']  # Target variable\n",
    "print(\"Train data shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import CatBoostEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Create a pipeline for preprocessing\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical),  # StandardScaler for numeric features\n",
    "        ('cat', CatBoostEncoder(), categorical)  # OneHotEncoder for categorical features\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Define classifiers\n",
    "rfc = RandomForestClassifier()\n",
    "gb = GradientBoostingClassifier()\n",
    "logistic = LogisticRegression(penalty='l2',max_iter = 1000)\n",
    "svm_rbf = SVC(kernel='rbf')\n",
    "\n",
    "# Create pipelines for classifiers with the updated ColumnTransformer\n",
    "rfc_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"Random Forest\", rfc)\n",
    "])\n",
    "\n",
    "gb_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"Gradient Boosting\", gb)\n",
    "])\n",
    "\n",
    "logistic_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"Logistic Regression\", logistic)\n",
    "])\n",
    "\n",
    "svm_rbf_pipeline = Pipeline([\n",
    "    (\"Data Transformations\", ct),\n",
    "    (\"SVM with RBF kernel\", svm_rbf)\n",
    "])\n",
    "\n",
    "# Fit and use the pipelines as before\n",
    "rfc_pipeline.fit(X_train, y_train)\n",
    "gb_pipeline.fit(X_train, y_train)\n",
    "logistic_pipeline.fit(X_train, y_train)\n",
    "svm_rbf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# After fitting, you can use the pipelines for prediction and evaluation\n",
    "rfc_predictions = rfc_pipeline.predict(X_test)\n",
    "gb_predictions = gb_pipeline.predict(X_test)\n",
    "logistic_predictions = logistic_pipeline.predict(X_test)\n",
    "svm_rbf_predictions = svm_rbf_pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "set_config(display =\"diagram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boost Training Accuracy:0.8440\n",
      "Gradient boost Test Accuracy:0.8533\n"
     ]
    }
   ],
   "source": [
    "gb_train_accuracy= gb_pipeline.score(X_train, y_train)\n",
    "print(f\"Gradient boost Training Accuracy:{gb_train_accuracy:.4f}\")\n",
    "gb_test_accuracy= gb_pipeline.score(X_test, y_test)\n",
    "print(f\"Gradient boost Test Accuracy:{gb_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy:0.8730\n",
      "Random Forest Test Accuracy:0.8533\n"
     ]
    }
   ],
   "source": [
    "rfc_train_accuracy= rfc_pipeline.score(X_train, y_train)\n",
    "print(f\"Random Forest Training Accuracy:{rfc_train_accuracy:.4f}\")\n",
    "rfc_test_accuracy= rfc_pipeline.score(X_test, y_test)\n",
    "print(f\"Random Forest Test Accuracy:{rfc_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Ridge Training Accuracy:0.8420\n",
      "Logistic Ridge Test Accuracy:0.8467\n"
     ]
    }
   ],
   "source": [
    "logistic_train_accuracy= logistic_pipeline.score(X_train, y_train)\n",
    "print(f\"Logistic Ridge Training Accuracy:{logistic_train_accuracy:.4f}\")\n",
    "logistic_test_accuracy= logistic_pipeline.score(X_test, y_test)\n",
    "print(f\"Logistic Ridge Test Accuracy:{logistic_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy:0.8630\n",
      "SVM Test Accuracy:0.8333\n"
     ]
    }
   ],
   "source": [
    "svm_train_accuracy= svm_rbf_pipeline.score(X_train, y_train)\n",
    "print(f\"SVM Training Accuracy:{svm_train_accuracy:.4f}\")\n",
    "svm_test_accuracy= svm_rbf_pipeline.score(X_test, y_test)\n",
    "print(f\"SVM Test Accuracy:{svm_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\"--rfc_n_estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--rfc_min_samples_split\", type=float, default=0.05)\n",
    "    parser.add_argument(\"--rfc_criterion\", type=str, default=\"gini\")\n",
    "    parser.add_argument(\"--gb_n_estimators\", type=int, default=100)\n",
    "    parser.add_argument(\"--gb_learning_rate\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--gb_max_depth\", type=int, default=3)\n",
    "    parser.add_argument(\"--logistic_max_iter\", type=int, default=1000)\n",
    "    parser.add_argument(\"--svm_kernel\", type=str, default=\"rbf\")\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    train_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/salary-prediction/train_data_salary_pred.csv',nrows=1000)  # Path to your train data file\n",
    "    test_df = pd.read_csv('s3://sagemaker-us-east-1-635439539142/akash/salary-prediction/test_data_salary_pred.csv',nrows=300)   # Path to your test data file\n",
    "\n",
    "    X_train = train_df.drop(\"salary\", axis=1)\n",
    "    y_train = train_df[\"salary\"]\n",
    "    X_test = test_df.drop(\"salary\", axis=1)\n",
    "    y_test = test_df[\"salary\"] \n",
    "\n",
    "    def data_type(dataset):\n",
    "        \"\"\"\n",
    "        Function to identify the numerical and categorical data columns\n",
    "        :param dataset: Dataframe\n",
    "        :return: list of numerical and categorical columns\n",
    "        \"\"\"\n",
    "        numerical = []\n",
    "        categorical = []\n",
    "        for i in dataset.columns:\n",
    "            if dataset[i].dtype == 'int64' or dataset[i].dtype == 'float64':\n",
    "                numerical.append(i)\n",
    "        else:\n",
    "            categorical.append(i)\n",
    "        return numerical, categorical\n",
    "\n",
    "\n",
    "    numerical, categorical = data_type(X_train)\n",
    "\n",
    "    # Identifying the binary columns and ignoring them from scaling\n",
    "    def binary_columns(df):\n",
    "        \"\"\"\n",
    "        Generates a list of binary columns in a dataframe.\n",
    "        \"\"\"\n",
    "        binary_cols = []\n",
    "        for col in df.select_dtypes(include=['int', 'float']).columns:\n",
    "            unique_values = df[col].unique()\n",
    "            if np.in1d(unique_values, [0, 1]).all():\n",
    "                binary_cols.append(col)\n",
    "        return binary_cols\n",
    "\n",
    "    binary_cols = binary_columns(X_train)\n",
    "\n",
    "    # Remove the binary columns from the numerical columns\n",
    "    numerical = [i for i in numerical if i not in binary_cols]\n",
    "    \n",
    "    # Define your encoder\n",
    "    ct = ColumnTransformer([\n",
    "        (\"CatBoostEncoding\", CatBoostEncoder(), categorical),\n",
    "        (\"Scaling\", StandardScaler(), numerical)\n",
    "    ])\n",
    "    \n",
    "    # Define classifiers\n",
    "    rfc = RandomForestClassifier(n_estimators=args.rfc_n_estimators, \n",
    "                                  min_samples_split=args.rfc_min_samples_split, \n",
    "                                  criterion=args.rfc_criterion)\n",
    "    \n",
    "    gb = GradientBoostingClassifier(n_estimators=args.gb_n_estimators, \n",
    "                                    learning_rate=args.gb_learning_rate, \n",
    "                                    max_depth=args.gb_max_depth)\n",
    "    \n",
    "    logistic = LogisticRegression(penalty='l2', max_iter=args.logistic_max_iter)\n",
    "    \n",
    "    svm_rbf = SVC(kernel=args.svm_kernel)\n",
    "    \n",
    "    # Create pipelines for classifiers with your encoder\n",
    "    rfc_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"Random Forest\", rfc)\n",
    "    ])\n",
    "\n",
    "    gb_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"Gradient Boosting\", gb)\n",
    "    ])\n",
    "\n",
    "    logistic_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"Logistic Regression\", logistic)\n",
    "    ])\n",
    "\n",
    "    svm_rbf_pipeline = Pipeline([\n",
    "        (\"Data Transformations\", ct),\n",
    "        (\"SVM with RBF kernel\", svm_rbf)\n",
    "    ])\n",
    "    \n",
    "    # Fit and evaluate each pipeline\n",
    "    for pipeline, name in [(rfc_pipeline, 'Random Forest'), (gb_pipeline, 'Gradient Boosting'), \n",
    "                           (logistic_pipeline, 'Logistic Regression'), (svm_rbf_pipeline, 'SVM with RBF kernel')]:\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        train_accuracy = pipeline.score(X_train, y_train)\n",
    "        test_accuracy = pipeline.score(X_test, y_test)\n",
    "        print(f\"{name} Training Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"{name} Test Accuracy: {test_accuracy:.4f}\")\n",
    "        \n",
    "        # Save the model\n",
    "        model_save_path = os.path.join(args.model_dir, f\"{name.lower().replace(' ', '_')}_model.joblib\")\n",
    "        joblib.dump(pipeline, model_save_path)\n",
    "        print(f\"Model Saved At: {model_save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "pandas\n",
    "scikit-learn\n",
    "fsspec\n",
    "category_encoders\n",
    "s3fs\n",
    "botocore==1.27.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Accuracy: 0.8510\n",
      "Random Forest Test Accuracy: 0.8500\n",
      "Model Saved At: ../models/random_forest_model.joblib\n",
      "Gradient Boosting Training Accuracy: 0.8750\n",
      "Gradient Boosting Test Accuracy: 0.8367\n",
      "Model Saved At: ../models/gradient_boosting_model.joblib\n",
      "Logistic Regression Training Accuracy: 0.8140\n",
      "Logistic Regression Test Accuracy: 0.8200\n",
      "Model Saved At: ../models/logistic_regression_model.joblib\n",
      "SVM with RBF kernel Training Accuracy: 0.8370\n",
      "SVM with RBF kernel Test Accuracy: 0.8300\n",
      "Model Saved At: ../models/svm_with_rbf_kernel_model.joblib\n"
     ]
    }
   ],
   "source": [
    "!python train.py --model_dir ../models/ --rfc_n_estimators 100 --rfc_min_samples_split 0.05 --rfc_criterion gini --gb_n_estimators 100 --gb_learning_rate 0.1 --gb_max_depth 3 --logistic_max_iter 1000 --svm_kernel rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: pipeline-run-2024-03-03-08-34-54-446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2024-03-03 08:34:54 Starting - Starting the training job...\n",
      "2024-03-03 08:35:10 Starting - Preparing the instances for training...\n",
      "2024-03-03 08:35:49 Downloading - Downloading input data...\n",
      "2024-03-03 08:36:12 Downloading - Downloading the training image...\n",
      "2024-03-03 08:36:52 Training - Training image download completed. Training in progress...\u001b[34m2024-03-03 08:36:57,925 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2024-03-03 08:36:57,929 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 08:36:57,977 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-03-03 08:36:58,169 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /miniconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /miniconda3/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (0.23.2)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 kB 16.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting category_encoders\n",
      "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.9/81.9 kB 23.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2023.1.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore==1.27.18 in /miniconda3/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (1.27.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /miniconda3/lib/python3.7/site-packages (from botocore==1.27.18->-r requirements.txt (line 6)) (1.26.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /miniconda3/lib/python3.7/site-packages (from botocore==1.27.18->-r requirements.txt (line 6)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /miniconda3/lib/python3.7/site-packages (from botocore==1.27.18->-r requirements.txt (line 6)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /miniconda3/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (2022.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.15.4 in /miniconda3/lib/python3.7/site-packages (from pandas->-r requirements.txt (line 1)) (1.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /miniconda3/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=0.19.1 in /miniconda3/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /miniconda3/lib/python3.7/site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mCollecting statsmodels>=0.9.0\n",
      "  Downloading statsmodels-0.13.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 107.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mCollecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.6-py2.py3-none-any.whl (233 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.9/233.9 kB 51.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.4.2\n",
      "  Downloading aiobotocore-2.4.2-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 19.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (987 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 988.0/988.0 kB 92.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.16.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.5/77.5 kB 21.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.11.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 139.5/139.5 kB 36.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.4.0\n",
      "  Downloading aiobotocore-2.4.1-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 13.7 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.4.0-py3-none-any.whl (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.8/65.8 kB 2.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.10.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.10.0-py3-none-any.whl (138 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.8/138.8 kB 36.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.8.2-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.8/140.8 kB 34.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.8.1-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.8.1-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.8/140.8 kB 33.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.8.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.8.0-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.0/141.0 kB 31.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.7.1-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.2/141.2 kB 31.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.3.4\n",
      "  Downloading aiobotocore-2.3.4-py3-none-any.whl (64 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.7/64.7 kB 14.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.7.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.7.0-py3-none-any.whl (141 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.2/141.2 kB 36.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.5.0-py3-none-any.whl (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 35.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.3.0\n",
      "  Downloading aiobotocore-2.3.3.tar.gz (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.7/65.7 kB 6.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.3.2.tar.gz (104 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.8/104.8 kB 19.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.3.1.tar.gz (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.3/65.3 kB 17.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.3.0.tar.gz (65 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.1/65.1 kB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of fsspec to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.3.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.2.0\n",
      "  Downloading aiobotocore-2.2.0.tar.gz (59 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.7/59.7 kB 7.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 136.1/136.1 kB 28.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.2.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.2-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.0/56.0 kB 12.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.9/134.9 kB 27.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 17.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.6/54.6 kB 12.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.2/133.2 kB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.11.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=2.0.1\n",
      "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.5/54.5 kB 16.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.0/133.0 kB 25.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.4/132.4 kB 35.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.5/52.5 kB 15.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.3/52.3 kB 1.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.10.1-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.6/125.6 kB 32.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.10.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.10.0-py3-none-any.whl (125 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.0/125.0 kB 33.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.9.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.6/123.6 kB 31.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 119.3/119.3 kB 34.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore~=1.4.0\n",
      "  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 51.6/51.6 kB 14.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.8.0-py3-none-any.whl (26 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.1/118.1 kB 31.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-2.6.0-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 22.1 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.4-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 15.9 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.3-py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.3/73.3 kB 20.3 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.2-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.9/72.9 kB 20.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34m  Downloading aiobotocore-2.5.1-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.8/72.8 kB 17.4 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.5.0-py3-none-any.whl (72 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.7/72.7 kB 1.6 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 15.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.6/50.6 kB 11.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 kB 11.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 11.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.2/48.2 kB 10.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.1/48.1 kB 10.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.2.1.tar.gz (48 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.0/48.0 kB 10.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\n",
      "2024-03-03 08:37:23 Uploading - Uploading generated training model\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.2.0.tar.gz (47 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 47.3/47.3 kB 8.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "  Downloading aiobotocore-1.1.2-py3-none-any.whl (45 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 kB 10.2 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.1.1-py3-none-any.whl (45 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.0/45.0 kB 12.7 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.1.0-py3-none-any.whl (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 12.5 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.7-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.9/42.9 kB 11.6 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.6-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 kB 8.7 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.5-py3-none-any.whl (42 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.1/42.1 kB 10.8 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.4-py3-none-any.whl (41 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.6/41.6 kB 1.2 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 11.5 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.2-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 10.1 MB/s eta 0:00:00\n",
      "  Downloading aiobotocore-1.0.1-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.7/40.7 kB 11.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.6.1-py3-none-any.whl (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.6.1-py3-none-any.whl (115 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 115.1/115.1 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.6.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.6.0-py3-none-any.whl (114 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.7/114.7 kB 28.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.5.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.5.0-py3-none-any.whl (111 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 111.7/111.7 kB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-2021.4.0-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mCollecting fsspec\n",
      "  Downloading fsspec-2021.4.0-py3-none-any.whl (108 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.3/108.3 kB 20.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting s3fs\n",
      "  Downloading s3fs-0.6.0-py3-none-any.whl (23 kB)\n",
      "  Downloading s3fs-0.5.2-py3-none-any.whl (22 kB)\n",
      "  Downloading s3fs-0.5.1-py3-none-any.whl (21 kB)\n",
      "  Downloading s3fs-0.5.0-py3-none-any.whl (21 kB)\n",
      "  Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /miniconda3/lib/python3.7/site-packages (from patsy>=0.5.1->category_encoders->-r requirements.txt (line 4)) (1.15.0)\u001b[0m\n",
      "\u001b[34mCollecting packaging>=21.3\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 12.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=3.1.0 in /miniconda3/lib/python3.7/site-packages (from importlib-resources->category_encoders->-r requirements.txt (line 4)) (3.13.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: patsy, packaging, importlib-resources, fsspec, statsmodels, s3fs, category_encoders\u001b[0m\n",
      "\u001b[34mSuccessfully installed category_encoders-2.6.3 fsspec-2023.1.0 importlib-resources-5.12.0 packaging-23.2 patsy-0.5.6 s3fs-0.4.2 statsmodels-0.13.5\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-03-03 08:37:16,560 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 08:37:16,574 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 08:37:16,587 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-03-03 08:37:16,597 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"gb_learning_rate\": 0.1,\n",
      "        \"gb_max_depth\": 3,\n",
      "        \"gb_n_estimators\": 100,\n",
      "        \"logistic_max_iter\": 1000,\n",
      "        \"rfc_criterion\": \"gini\",\n",
      "        \"rfc_min_samples_split\": 0.05,\n",
      "        \"rfc_n_estimators\": 100,\n",
      "        \"svm_kernel\": \"rbf\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pipeline-run-2024-03-03-08-34-54-446\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-08-34-54-446/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"gb_learning_rate\":0.1,\"gb_max_depth\":3,\"gb_n_estimators\":100,\"logistic_max_iter\":1000,\"rfc_criterion\":\"gini\",\"rfc_min_samples_split\":0.05,\"rfc_n_estimators\":100,\"svm_kernel\":\"rbf\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-08-34-54-446/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"gb_learning_rate\":0.1,\"gb_max_depth\":3,\"gb_n_estimators\":100,\"logistic_max_iter\":1000,\"rfc_criterion\":\"gini\",\"rfc_min_samples_split\":0.05,\"rfc_n_estimators\":100,\"svm_kernel\":\"rbf\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pipeline-run-2024-03-03-08-34-54-446\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-08-34-54-446/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--gb_learning_rate\",\"0.1\",\"--gb_max_depth\",\"3\",\"--gb_n_estimators\",\"100\",\"--logistic_max_iter\",\"1000\",\"--rfc_criterion\",\"gini\",\"--rfc_min_samples_split\",\"0.05\",\"--rfc_n_estimators\",\"100\",\"--svm_kernel\",\"rbf\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_GB_LEARNING_RATE=0.1\u001b[0m\n",
      "\u001b[34mSM_HP_GB_MAX_DEPTH=3\u001b[0m\n",
      "\u001b[34mSM_HP_GB_N_ESTIMATORS=100\u001b[0m\n",
      "\u001b[34mSM_HP_LOGISTIC_MAX_ITER=1000\u001b[0m\n",
      "\u001b[34mSM_HP_RFC_CRITERION=gini\u001b[0m\n",
      "\u001b[34mSM_HP_RFC_MIN_SAMPLES_SPLIT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_RFC_N_ESTIMATORS=100\u001b[0m\n",
      "\u001b[34mSM_HP_SVM_KERNEL=rbf\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python train.py --gb_learning_rate 0.1 --gb_max_depth 3 --gb_n_estimators 100 --logistic_max_iter 1000 --rfc_criterion gini --rfc_min_samples_split 0.05 --rfc_n_estimators 100 --svm_kernel rbf\u001b[0m\n",
      "\u001b[34mRandom Forest Training Accuracy: 0.8590\u001b[0m\n",
      "\u001b[34mRandom Forest Test Accuracy: 0.8367\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/random_forest_model.joblib\u001b[0m\n",
      "\u001b[34mGradient Boosting Training Accuracy: 0.8750\u001b[0m\n",
      "\u001b[34mGradient Boosting Test Accuracy: 0.8400\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/gradient_boosting_model.joblib\u001b[0m\n",
      "\u001b[34mLogistic Regression Training Accuracy: 0.8140\u001b[0m\n",
      "\u001b[34mLogistic Regression Test Accuracy: 0.8200\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/logistic_regression_model.joblib\u001b[0m\n",
      "\u001b[34mSVM with RBF kernel Training Accuracy: 0.8370\u001b[0m\n",
      "\u001b[34mSVM with RBF kernel Test Accuracy: 0.8300\u001b[0m\n",
      "\u001b[34mModel Saved At: /opt/ml/model/svm_with_rbf_kernel_model.joblib\u001b[0m\n",
      "\u001b[34m2024-03-03 08:37:18,950 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-03-03 08:37:38 Completed - Training job completed\n",
      "Training seconds: 109\n",
      "Billable seconds: 44\n",
      "Managed Spot Training savings: 59.6%\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    base_job_name=\"pipeline-run\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    entry_point=\"train.py\",\n",
    "    dependencies=[\"requirements.txt\"],  # Include the requirements file\n",
    "    hyperparameters={\n",
    "        \"rfc_n_estimators\": 100,\n",
    "        \"rfc_min_samples_split\": 0.05,\n",
    "        \"rfc_criterion\": \"gini\",\n",
    "        \"gb_n_estimators\": 100,\n",
    "        \"gb_learning_rate\": 0.1,\n",
    "        \"gb_max_depth\": 3,\n",
    "        \"logistic_max_iter\": 1000,\n",
    "        \"svm_kernel\": \"rbf\"\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    use_spot_instances=True,\n",
    "    max_wait=600,\n",
    "    max_run=600,\n",
    "    role=get_execution_role(),\n",
    ")\n",
    "\n",
    "sklearn_estimator.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name : pipeline-run-2024-03-03-08-34-54-446\n",
      "Model storage location : s3://sagemaker-us-east-1-635439539142/pipeline-run-2024-03-03-08-34-54-446/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "training_job_name = sklearn_estimator.latest_training_job.name\n",
    "model_artifact = sm_client.describe_training_job(\n",
    "    TrainingJobName = training_job_name\n",
    ")[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "print(f\"Training job name : {training_job_name}\")\n",
    "print(f\"Model storage location : {model_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/train_df.csv')\n",
    "test_df.to_csv('../data/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating hyperparameter tuning job with name: pipeline-run-240303-0855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Define the session, role, and S3 bucket\n",
    "role = get_execution_role()\n",
    "\n",
    "# Define hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"rfc_n_estimators\": IntegerParameter(50, 150),\n",
    "    \"rfc_min_samples_split\": ContinuousParameter(0.01, 0.5),\n",
    "    \"rfc_criterion\": CategoricalParameter([\"gini\", \"entropy\"]),\n",
    "    \"gb_n_estimators\": IntegerParameter(50, 150),\n",
    "    \"gb_learning_rate\": ContinuousParameter(0.01, 0.5),\n",
    "    \"gb_max_depth\": IntegerParameter(3, 10),\n",
    "    \"logistic_max_iter\": IntegerParameter(100, 1000),\n",
    "    \"svm_kernel\": CategoricalParameter([\"linear\", \"poly\", \"rbf\", \"sigmoid\"])\n",
    "}\n",
    "\n",
    "# Define the objective metric name and type\n",
    "objective_metric_name = 'Training_Accuracy'\n",
    "objective_type = 'Maximize'\n",
    "\n",
    "# Define the metric definitions function\n",
    "def generate_metric_definitions():\n",
    "    return [\n",
    "        {\n",
    "            \"Name\": \"Training_Accuracy\",\n",
    "            \"Regex\": \"[a-zA-Z].*\\\\s+Training\\\\s+Accuracy:\\\\s+([0-9\\\\.]+)\"\n",
    "        },\n",
    "        {\n",
    "            \"Name\": \"Test_Accuracy\",\n",
    "            \"Regex\": \"[a-zA-Z].*\\\\s+Test\\\\s+Accuracy:\\\\s+([0-9\\\\.]+)\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "# Define the HyperparameterTuner\n",
    "tuner = HyperparameterTuner(\n",
    "    base_tuning_job_name='pipeline-run',\n",
    "    estimator=sklearn_estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    objective_type=objective_type,\n",
    "    metric_definitions=generate_metric_definitions(),\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2\n",
    ")\n",
    "\n",
    "# Launch the hyperparameter tuning job\n",
    "tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 15)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_job_name = tuner.latest_tuning_job.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = tuner.best_estimator().hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_analytics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuning_metrics = tuner_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot hyperparameter vs. objective metric\n",
    "plt.scatter(tuning_metrics['parameter_<hyperparameter_name>'], tuning_metrics['FinalObjectiveValue'])\n",
    "plt.xlabel('<Hyperparameter Name>')\n",
    "plt.ylabel('Objective Metric Value')\n",
    "plt.title('Hyperparameter Tuning Results')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job_name in tuner.analytics().training_job_summaries:\n",
    "    print(f\"Training Job Name: {job_name['TrainingJobName']}, Objective Metric Value: {job_name['FinalObjectiveValue']}\")\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
